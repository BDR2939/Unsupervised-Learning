{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 001 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.001 | 02/04/2023 | Royi Avital | Fixed a typo in question `0.1.`                                    |\n",
    "| 0.1.000 | 12/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0001Part001.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names of the team memebers in the `Team Members` section.\n",
    " - Answer all questions within the Jupyter Notebook.\n",
    " - Open questions are in part I of the exercise.\n",
    " - Coding based questions are in the subsequent notebooks.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for question.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    " - `<FULL>_<NAME>_<001>`.\n",
    " - `<FULL>_<NAME>_<002>`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Linear Algebra\n",
    "\n",
    "A matrix $ P $ is called an orthogonal projection operator if, and only if it is idempotent and symmetric.\n",
    "\n",
    "**Remark**: Idempotent matrix means $ \\forall n \\in \\mathcal{N} \\; {P}^{n} = P $.\n",
    "\n",
    "### 0.1. Question\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ where $ m \\geq n $ and $ \\operatorname{Rank} \\left( A \\right) = n $.  \n",
    "Given the linear least squares problem:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "With the solution in the form $\\hat{\\boldsymbol{x}} = R \\boldsymbol{y}$, show that $P = A R$ is an orthogonal projection operator.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "1. Derive the solution to the Least Squares above in the form of $ \\hat{\\boldsymbol{x}} = R \\boldsymbol{y} $.\n",
    "2. Show the $ P $ matrix is symmetric.\n",
    "3. Show the $ P $ matrix is idempotent.\n",
    "4. Conclude the matrix is an orthogonal projection operator.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The matrix $P$ is the Orthogonal Projection onto the range (Columns space) of $ A $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Solution\n",
    "\n",
    "let $ S\\left(x\\right) =: \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} = \\frac{1}{2} {\\left( A \\boldsymbol{x} - \\boldsymbol{y} \\right)^{2}} = \\frac{1}{2}\\left( A \\boldsymbol{x} - \\boldsymbol{y}\\right)^{T}\\left( A \\boldsymbol{x} - \\boldsymbol{y}\\right)\\Rightarrow \\frac{dS}{dx} = A^{T}\\left(Ax-y\\right) \\Rightarrow \\text{to find the minimum we want} \\; \\frac{dS}{dx} = 0 \\\\ \\text{hence} \\; A^{T}\\left(Ax-y\\right) = 0 \\Rightarrow A^{T}Ax=A^{T}y \\Rightarrow x = \\left(A^{T}A\\right)^{-1}A^{T}y\\\\$\n",
    "$$\\Downarrow$$\n",
    "$$P = A\\left(A^{T}A\\right)^{-1}A^{T}$$\n",
    "$$P^{T} = \\left(A\\left(A^{T}A\\right)^{-1}A^{T}\\right)^{T} = A\\left(\\left(A^{T}A\\right)^{-1}\\right)^{T}A^{T} = A\\left(\\left(A^{T}A\\right)^{T}\\right)^{-1}A^{T} = A\\left(A^{T}A\\right)^{-1}A^{T} = P$$\n",
    "$$P^{2} = A\\left(A^{T}A\\right)^{-1}A^{T}A\\left(A^{T}A\\right)^{-1}A^{T} = A\\cancel{\\left(A^{T}A\\right)^{-1}}\\cancel{A^{T}A}\\left(A^{T}A\\right)^{-1}A^{T} =  A\\left(A^{T}A\\right)^{-1}A^{T}  = P$$\n",
    "\n",
    "Given $P$ is idempotent and symmetric as shown above, it is an orthogonal projection operator\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convexity\n",
    "\n",
    "**Convex Set**  \n",
    "\n",
    "Let:\n",
    "\n",
    "$$ \\mathbb{R}_{\\geq 0}^{d} = \\left\\{ \\boldsymbol{x} \\in\\mathbb{R}^{d} \\, \\bigg| \\, \\min_{i} {x}_{i} \\geq 0 \\right\\} $$\n",
    "\n",
    "Where $\\boldsymbol{x} = \\begin{bmatrix} {x}_{1} \\\\ {x}_{2} \\\\ \\vdots \\\\ {x}_{d} \\end{bmatrix}$\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Prove or disprove that $\\mathbb{R}_{\\geq 0}^{d}$ is convex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Solution\n",
    "\n",
    "let $x, y \\in \\mathbb{R}_{\\geq 0}^{d}$, $\\alpha \\in \\left[0, 1\\right]$. <br>\n",
    "$\\underbrace{\\alpha x}_{\\geq 0} + \\underbrace{\\left(1-\\alpha \\right)y}_{\\geq 0} \\Rightarrow \\alpha x + \\left(1-\\alpha \\right)y \\in \\mathbb{R}_{\\geq 0}^{d}$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convex Combination** \n",
    "\n",
    "Let $\\mathcal{C} \\subseteq \\mathbb{R}^{d} $ be a convex set and consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\} _{i=1}^{N}$.\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Prove that for any $N \\in \\mathbb{N}$: \n",
    "\n",
    "$$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $$\n",
    "\n",
    "Where $\\alpha_{i}$ are such that: \n",
    "\n",
    " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
    " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The properties of ${\\alpha}_{i}$ above means it is sampled from the Unit Probability Simplex.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "Prove by induction:\n",
    "\n",
    "For 1 it's clear. <br>\n",
    "\n",
    "For 2 by the demand of $\\sum_{i = 1}^{N} \\alpha_{i} = 1$ we get $\\alpha_2 = \\left(1-\\alpha_1\\right)$, hence, by the definition of a convex set $\\alpha_1 x_1 + \\alpha_2 x_2 \\in \\mathcal{C}$ <br>\n",
    "\n",
    "Assume it exists for n.<br>\n",
    "Proof for n+1:<br>\n",
    "\n",
    "$\\sum_{i = 1}^{n+1} {\\alpha}_{i} \\boldsymbol{x}_{i} = \\sum_{i = 1}^{n} {\\alpha}_{i} \\boldsymbol{x}_{i} + {\\alpha}_{n+1} \\boldsymbol{x}_{n+1}$ <br>\n",
    "\n",
    "where $\\alpha_{n+1} = 1 - \\sum_{i = 1}^{n} {\\alpha}_{i}$ <br>\n",
    "\n",
    "Now we can write it this way:<br>\n",
    "\n",
    "$\\left(\\sum_{i = 1}^{n} {\\alpha}_{i}\\right) \\left(\\sum_{i = 1}^{n} \\frac{\\alpha_i}{\\sum_{j = 1}^{n} {\\alpha}_{j}}x_i \\right) + \\left(1 - \\sum_{i = 1}^{n} {\\alpha}_{i}\\right)x_{n+1}$ <br>\n",
    "\n",
    "Which is a convex combination of 2 points in the set, hence in the set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{C}\\subset\\mathbb{R}^{2}$ be a convex set.  \n",
    "Consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ such that $\\boldsymbol{x}_{i} \\neq \\boldsymbol{x}_{j}$ for all $i \\neq j$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Question\n",
    "\n",
    "Prove or disprove the following assertion:\n",
    "\n",
    "Necessarily, any point $\\boldsymbol{y} \\in \\mathcal{C}$ can be represented as a convex combination of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Solution\n",
    "\n",
    "let:<br>\n",
    "\n",
    "$x_k \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ such that $\\forall x_{i}^{1} \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10} x_{k}^{1} \\leq x_{i}^{1}$ <br>\n",
    "\n",
    "$x_l \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ such that $\\forall x_{i}^{1} \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10} x_{l}^{1} \\geq x_{i}^{1}$ \n",
    "\n",
    "$x_m \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ such that $\\forall x_{i}^{2} \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10} x_{m}^{2} \\leq x_{i}^{2}$ <br>\n",
    "\n",
    "$x_n \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ such that $\\forall x_{i}^{2} \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10} x_{n}^{2} \\geq x_{i}^{2}$ <br>\n",
    "\n",
    "convex combination fulffils the following:<br>\n",
    "\n",
    " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
    " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
    "\n",
    "meaning a convex combination of the $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ is in the convex hull of these points.\n",
    "\n",
    "let $y \\in \\mathcal{C}$ such that $y^{1} \\geq x_l^{1}, y^{2} \\geq x_n^{2}$ <br>\n",
    "\n",
    "$y$ is not in the convex hull of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$, hence no convex combination of these points can represent $y$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Gradient\n",
    "\n",
    "**Remark**: Assume all functions in this section are differentiable.\n",
    "\n",
    "\n",
    "**Directional Derivative**\n",
    "\n",
    "Let $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ and let $\\boldsymbol{x}_{0} \\in \\mathbb{R}^{d}$. \n",
    "\n",
    "### 2.1. Question\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$ \\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle \\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x}_{0} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. Solution\n",
    "\n",
    "$$\\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = ֿ\\lim_{t \\to 0}\\frac{f\\left(x_0+th\\right) - f\\left(x_0\\right)}{t} \\underbrace{=}_{\\text{Taylor's theorem}} ֿ\\lim_{t \\to 0}\\frac{f\\left(x_0\\right)+\\nabla f\\left(x_0\\right)\\cdot\\left(th\\right) + o\\left(t\\right) - f\\left(x_0\\right)}{t} = \\\\\n",
    "ֿ\\\\\n",
    "\\lim_{t \\to 0}\\frac{t \\cdot \\nabla f\\left(x_0\\right)\\cdot\\left(h\\right) + o\\left(t\\right)}{t} = \\lim_{t \\to 0}\\frac{t \\cdot \\nabla f\\left(x_0\\right)\\cdot\\left(h\\right)}{t} + \\lim_{t \\to 0}\\frac{o\\left(t\\right)}{t} = \\lim_{t \\to 0}\\nabla f\\left(x_0\\right)\\cdot\\left(h\\right) = \\nabla f\\left(x_0\\right)\\cdot\\left(h\\right) = \\left<\\nabla f\\left(x_0\\right), h\\right> \\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x}_{0} \\right) $$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**\n",
    "\n",
    "$f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ is said to be **linear** if:\n",
    "\n",
    "$$ f \\left( \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y} \\right) = \\alpha f \\left( \\boldsymbol{x} \\right) + \\beta f \\left( \\boldsymbol{y} \\right) $$\n",
    "\n",
    "For all $\\alpha, \\beta \\in \\mathbb{R}$ and for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{{d}_{1}}$.\n",
    "\n",
    "\n",
    "\n",
    "Let $f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ be a linear function.\n",
    "\n",
    "### 2.2. Question\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = f \\left( \\boldsymbol{h} \\right) $$\n",
    "\n",
    "For all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{{d}_{1}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solution\n",
    "Since f is linear:\n",
    "\n",
    "$p(t) = f(x+th) = f(x) + tf(h)$\n",
    "\n",
    "therfore:\n",
    "\n",
    "$\\nabla f(x)[h] = p'(0) = f(h)$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Solution\n",
    "\n",
    "Let $f(x) = x^{T}Ax=\\:<x, Ax>$ \n",
    "\n",
    "then:\n",
    "\n",
    "$\\nabla f(x)[h]=\\nabla<x, Ax>[h] = \\: <h, Ax> + <x, Ah> \\: =\\\\\n",
    "<Ax+A^{T}x, h>$\n",
    "\n",
    "therfore: $ \\nabla f(x) = Ax+A^{T}x$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{A} \\boldsymbol{X} \\right\\} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Solution\n",
    "\n",
    "Let $f(X) = Tr\\{X^{T}AX\\} = \\: <X, AX>$ \n",
    "\n",
    "then:\n",
    "\n",
    "$\\nabla f(X)[H]=\\nabla<X, AX>[H] = \\: <H, AX> + <X, AH> \\: =\\\\\n",
    "<AX+A^{T}X, H>$\n",
    "\n",
    "therfore: $ \\nabla f(X) = AX+A^{T}X$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Solution\n",
    "\n",
    "Let $f(x) = ||y-Ax||^{2}_{2} = g_2(g_1(x))$ \n",
    "\n",
    "where:\n",
    "\n",
    "$g_1(x) = y-Ax$   \n",
    "\n",
    "$g_2(x) = ||x||^2_2$\n",
    "\n",
    "and:\n",
    "\n",
    "$\\nabla g_1(x)[h] = -Ah $   \n",
    "\n",
    "$\\nabla g_2(x) = 2x$\n",
    "\n",
    "then:\n",
    "\n",
    "$ \\nabla f(x)[h]= \\ <\\nabla g_2(g_1(x)), \\nabla(g_1(x)[h]> \\ =\\ <-2A^{T}(y-Ax), h>$\n",
    "\n",
    "therfore: $\\nabla f(x) = -2A^{T}(y-Ax)$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X} \\right\\|}_{F}^{2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{D \\times d}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$.\n",
    " - ${\\left\\| \\cdot \\right\\|}_{F}^{2}$ is the squared [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm), that is, ${\\left\\| \\boldsymbol{X} \\right\\|}_{F}^{2} = \\left\\langle \\boldsymbol{X}, \\boldsymbol{X} \\right\\rangle = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{X} \\right\\}$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Solution\n",
    "\n",
    "Let $f(X) = ||Y-AX||^{2}_{F} = g_2(g_1(X))$ \n",
    "\n",
    "where:\n",
    "\n",
    "$g_1(X) = X-AX$   \n",
    "\n",
    "$g_2(X) = ||X||^2_F$\n",
    "\n",
    "and:\n",
    "\n",
    "$\\nabla g_1(X)[H] = -AH $   \n",
    "\n",
    "$\\nabla g_2(X)[H] =\\ <H, X> + <X, H>\\ =\\ <2X, H>$\n",
    "\n",
    "then:\n",
    "\n",
    "$ \\nabla f(X)[H]= \\ <\\nabla g_2(g_1(X)), \\nabla(g_1(X)[H]> \\ =\\ <-2A^{T}(Y-AX), H>$\n",
    "\n",
    "therfore: $\\nabla f(x) = -2A^{T}(Y-AX)$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{X}^{T} \\boldsymbol{A}, \\boldsymbol{Y}^{T} \\right\\rangle $$\n",
    "\n",
    "Where $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times D}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Solution\n",
    "\n",
    "$\\nabla f(X)[H] =\\ <H^{T}A, Y^{T}>\\ =\\ Tr(A^{T}HY^{T}) =\\ Tr(Y^{T}A^{T}H) = \\\\\n",
    "<(Y^{T}A^{T})^T, H>\\ =\\ <AY, H>$ \n",
    "\n",
    "therfore: $\\nabla f(X) = AY$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) $$\n",
    "\n",
    "Where $g \\left( \\cdot \\right)$ is an element wise function $g \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{A}, \\log \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\log \\left( \\cdot \\right)$ is the element wise $\\log$ function: $\\boldsymbol{M} = \\log \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\log \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constraint Optimization\n",
    "\n",
    "**MinMax**  \n",
    "\n",
    "Let $G \\left( x, y \\right) = \\sin \\left( x + y \\right)$.\n",
    "\n",
    "### 3.1. Question\n",
    "\n",
    "Show that:\n",
    "\n",
    " - $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$.\n",
    " - $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Solution\n",
    "\n",
    "\n",
    "\n",
    "*   for fixed x: $\\underset{y}{\\max} G \\left( x, y \\right) = 1$. \n",
    "\n",
    "therfore: $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = \\underset{x}{\\min}(1) = 1$.\n",
    "\n",
    "\n",
    "*   for fixed y: $\\underset{x}{\\min} G \\left( x, y \\right) = -1$. \n",
    "\n",
    "therfore: $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = \\underset{y}{\\max}(-1) = -1$.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rayleigh Quotient**  \n",
    "\n",
    "The _Rayleigh Quotient_ is defined by:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
    "\n",
    "For some symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "### 3.2. Question\n",
    "\n",
    "Follow the given steps:\n",
    "\n",
    " - Show that $ {\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) = \\begin{cases} {\\min}_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases} $.\n",
    " - Write the Lagrangian of the constraint objective $\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right)$.\n",
    " - Show that ${\\nabla}_{\\boldsymbol{x}} \\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x}$.  \n",
    "   In other words, the stationary points $\\left( \\boldsymbol{x}, \\lambda \\right)$ are the eigenvectors and eigenvalues of $\\boldsymbol{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/qIP5xPv.png\" height=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
