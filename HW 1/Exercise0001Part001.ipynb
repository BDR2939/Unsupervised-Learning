{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLOb9cYqxehB"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 001 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 0.1.001 | 02/04/2023 | Royi Avital | Fixed a typo in question `0.1.`                                    |\n",
        "| 0.1.000 | 12/03/2023 | Royi Avital | First version                                                      |\n",
        "|         |            |             |                                                                    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxCFAFvaxehG"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0001Part001.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_swczq6xehH"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcS2ZUiTxehI"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names of the team memebers in the `Team Members` section.\n",
        " - Answer all questions within the Jupyter Notebook.\n",
        " - Open questions are in part I of the exercise.\n",
        " - Coding based questions are in the subsequent notebooks.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for question.\n",
        " - Good Luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSl57SDDxehI"
      },
      "source": [
        "## Team Members\n",
        "\n",
        " - `<FULL>_<NAME>_<001>`.\n",
        " - `<FULL>_<NAME>_<002>`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jW_Q0NGxehJ"
      },
      "source": [
        "## 0. Linear Algebra\n",
        "\n",
        "A matrix $ P $ is called an orthogonal projection operator if, and only if it is idempotent and symmetric.\n",
        "\n",
        "**Remark**: Idempotent matrix means $ \\forall n \\in \\mathcal{N} \\; {P}^{n} = P $.\n",
        "\n",
        "### 0.1. Question\n",
        "\n",
        "Let $A \\in \\mathbb{R}^{m \\times n}$ where $ m \\geq n $ and $ \\operatorname{Rank} \\left( A \\right) = n $.  \n",
        "Given the linear least squares problem:\n",
        "\n",
        "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "With the solution in the form $\\hat{\\boldsymbol{x}} = R \\boldsymbol{y}$, show that $P = A R$ is an orthogonal projection operator.\n",
        "\n",
        "**Hints**\n",
        "\n",
        "1. Derive the solution to the Least Squares above in the form of $ \\hat{\\boldsymbol{x}} = R \\boldsymbol{y} $.\n",
        "2. Show the $ P $ matrix is symmetric.\n",
        "3. Show the $ P $ matrix is idempotent.\n",
        "4. Conclude the matrix is an orthogonal projection operator.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The matrix $P$ is the Orthogonal Projection onto the range (Columns space) of $ A $."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dCrSmJcWxehK"
      },
      "source": [
        "### 0.1. Solution\n",
        "\n",
        "let $ S\\left(x\\right) =: \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} = \\frac{1}{2} {\\left( A \\boldsymbol{x} - \\boldsymbol{y} \\right)^{2}} = \\frac{1}{2}\\left( A \\boldsymbol{x} - \\boldsymbol{y}\\right)^{T}\\left( A \\boldsymbol{x} - \\boldsymbol{y}\\right)\\Rightarrow \\frac{dS}{dx} = A^{T}\\left(Ax-y\\right) \\Rightarrow \\text{to find the minimum we want} \\; \\frac{dS}{dx} = 0 \\\\ \\text{hence} \\; A^{T}\\left(Ax-y\\right) = 0 \\Rightarrow A^{T}Ax=A^{T}y \\Rightarrow x = \\left(A^{T}A\\right)^{-1}A^{T}y\\\\$\n",
        "$$\\Downarrow$$\n",
        "$$P = A\\left(A^{T}A\\right)^{-1}A^{T}$$\n",
        "$$P^{T} = \\left(A\\left(A^{T}A\\right)^{-1}A^{T}\\right)^{T} = A\\left(\\left(A^{T}A\\right)^{-1}\\right)^{T}A^{T} = A\\left(\\left(A^{T}A\\right)^{T}\\right)^{-1}A^{T} = A\\left(A^{T}A\\right)^{-1}A^{T} = P$$\n",
        "$$P^{2} = A\\left(A^{T}A\\right)^{-1}A^{T}A\\left(A^{T}A\\right)^{-1}A^{T} = A\\cancel{\\left(A^{T}A\\right)^{-1}}\\cancel{A^{T}A}\\left(A^{T}A\\right)^{-1}A^{T} =  A\\left(A^{T}A\\right)^{-1}A^{T}  = P$$\n",
        "\n",
        "Given $P$ is idempotent and symmetric as shown above, it is an orthogonal projection operator\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIXmWvZExehL"
      },
      "source": [
        "## 1. Convexity\n",
        "\n",
        "**Convex Set**  \n",
        "\n",
        "Let:\n",
        "\n",
        "$$ \\mathbb{R}_{\\geq 0}^{d} = \\left\\{ \\boldsymbol{x} \\in\\mathbb{R}^{d} \\, \\bigg| \\, \\min_{i} {x}_{i} \\geq 0 \\right\\} $$\n",
        "\n",
        "Where $\\boldsymbol{x} = \\begin{bmatrix} {x}_{1} \\\\ {x}_{2} \\\\ \\vdots \\\\ {x}_{d} \\end{bmatrix}$\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Prove or disprove that $\\mathbb{R}_{\\geq 0}^{d}$ is convex."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "37jad3hFxehL"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "let $x, y \\in \\mathbb{R}_{\\geq 0}^{d}$, $\\alpha \\in \\left[0, 1\\right]$. <br>\n",
        "$\\underbrace{\\alpha x}_{\\geq 0} + \\underbrace{\\left(1-\\alpha \\right)y}_{\\geq 0} \\Rightarrow \\alpha x + \\left(1-\\alpha \\right)y \\in \\mathbb{R}_{\\geq 0}^{d}$\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MteqMZOCxehM"
      },
      "source": [
        "**Convex Combination** \n",
        "\n",
        "Let $\\mathcal{C} \\subseteq \\mathbb{R}^{d} $ be a convex set and consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\} _{i=1}^{N}$.\n",
        "\n",
        "### 1.2. Question\n",
        "\n",
        "Prove that for any $N \\in \\mathbb{N}$: \n",
        "\n",
        "$$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $$\n",
        "\n",
        "Where $\\alpha_{i}$ are such that: \n",
        "\n",
        " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
        " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The properties of ${\\alpha}_{i}$ above means it is sampled from the Unit Probability Simplex.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rS9OFZ4ixehM"
      },
      "source": [
        "### 1.2. Solution\n",
        "\n",
        "Prove by induction:\n",
        "\n",
        "For 1 it's clear. <br>\n",
        "\n",
        "For 2 by the demand of $\\sum_{i = 1}^{N} \\alpha_{i} = 1$ we get $\\alpha_2 = \\left(1-\\alpha_1\\right)$, hence, by the definition of a convex set $\\alpha_1 x_1 + \\alpha_2 x_2 \\in \\mathcal{C}$ <br>\n",
        "\n",
        "Assume it exists for n.<br>\n",
        "Proof for n+1:<br>\n",
        "\n",
        "$\\sum_{i = 1}^{n+1} {\\alpha}_{i} \\boldsymbol{x}_{i} = \\sum_{i = 1}^{n} {\\alpha}_{i} \\boldsymbol{x}_{i} + {\\alpha}_{n+1} \\boldsymbol{x}_{n+1}$ <br>\n",
        "\n",
        "where $\\alpha_{n+1} = 1 - \\sum_{i = 1}^{n} {\\alpha}_{i}$ <br>\n",
        "\n",
        "Now we can write it this way:<br>\n",
        "\n",
        "$\\left(\\sum_{i = 1}^{n} {\\alpha}_{i}\\right) \\left(\\sum_{i = 1}^{n} \\frac{\\alpha_i}{\\sum_{j = 1}^{n} {\\alpha}_{j}}x_i \\right) + \\left(1 - \\sum_{i = 1}^{n} {\\alpha}_{i}\\right)x_{n+1}$ <br>\n",
        "\n",
        "Which is a convex combination of 2 points in the set, hence in the set.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FswSDzylxehN"
      },
      "source": [
        "Let $\\mathcal{C}\\subset\\mathbb{R}^{2}$ be a convex set.  \n",
        "Consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ such that $\\boldsymbol{x}_{i} \\neq \\boldsymbol{x}_{j}$ for all $i \\neq j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GVn_8bOxehN"
      },
      "source": [
        "### 1.3. Question\n",
        "\n",
        "Prove or disprove the following assertion:\n",
        "\n",
        "Necessarily, any point $\\boldsymbol{y} \\in \\mathcal{C}$ can be represented as a convex combination of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Z7Xmh0xehN"
      },
      "source": [
        "### 1.3. Solution\n",
        "\n",
        "let:<br>\n",
        "\n",
        "$x_k \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ such that $\\forall x_{i}^{1} \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10} x_{k}^{1} \\leq x_{i}^{1}$ <br>\n",
        "\n",
        "$x_l \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ such that $\\forall x_{i}^{1} \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10} x_{l}^{1} \\geq x_{i}^{1}$ \n",
        "\n",
        "$x_m \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ such that $\\forall x_{i}^{2} \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10} x_{m}^{2} \\leq x_{i}^{2}$ <br>\n",
        "\n",
        "$x_n \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ such that $\\forall x_{i}^{2} \\in \\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10} x_{n}^{2} \\geq x_{i}^{2}$ <br>\n",
        "\n",
        "convex combination fulffils the following:<br>\n",
        "\n",
        " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
        " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
        "\n",
        "meaning a convex combination of the $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$ is in the convex hull of these points.\n",
        "\n",
        "let $y \\in \\mathcal{C}$ such that $y^{1} \\geq x_l^{1}, y^{2} \\geq x_n^{2}$ <br>\n",
        "\n",
        "$y$ is not in the convex hull of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$, hence no convex combination of these points can represent $y$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig4khdyFxehO"
      },
      "source": [
        "\n",
        "### 2.1. Solution\n",
        "\n",
        "$$\\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = ֿ\\lim_{t \\to 0}\\frac{f\\left(x_0+th\\right) - f\\left(x_0\\right)}{t} \\underbrace{=}_{\\text{Taylor's theorem}} ֿ\\lim_{t \\to 0}\\frac{f\\left(x_0\\right)+\\nabla f\\left(x_0\\right)\\cdot\\left(th\\right) + o\\left(t\\right) - f\\left(x_0\\right)}{t} = \\\\\n",
        "ֿ\\\\\n",
        "\\lim_{t \\to 0}\\frac{t \\cdot \\nabla f\\left(x_0\\right)\\cdot\\left(h\\right) + o\\left(t\\right)}{t} = \\lim_{t \\to 0}\\frac{t \\cdot \\nabla f\\left(x_0\\right)\\cdot\\left(h\\right)}{t} + \\lim_{t \\to 0}\\frac{o\\left(t\\right)}{t} = \\lim_{t \\to 0}\\nabla f\\left(x_0\\right)\\cdot\\left(h\\right) = \\nabla f\\left(x_0\\right)\\cdot\\left(h\\right) = \\left<\\nabla f\\left(x_0\\right), h\\right> \\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x}_{0} \\right) $$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4ZK50qRxehO"
      },
      "source": [
        "**Definition**\n",
        "\n",
        "$f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ is said to be **linear** if:\n",
        "\n",
        "$$ f \\left( \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y} \\right) = \\alpha f \\left( \\boldsymbol{x} \\right) + \\beta f \\left( \\boldsymbol{y} \\right) $$\n",
        "\n",
        "For all $\\alpha, \\beta \\in \\mathbb{R}$ and for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{{d}_{1}}$.\n",
        "\n",
        "\n",
        "\n",
        "Let $f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ be a linear function.\n",
        "\n",
        "### 2.2. Question\n",
        "\n",
        "Prove that:\n",
        "\n",
        "$$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = f \\left( \\boldsymbol{h} \\right) $$\n",
        "\n",
        "For all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{{d}_{1}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSYwQtJmxehP"
      },
      "source": [
        "### 2.2. Solution\n",
        "Since f is linear:\n",
        "\n",
        "$p(t) = f(x+th) = f(x) + tf(h)$\n",
        "\n",
        "therfore:\n",
        "\n",
        "$\\nabla f(x)[h] = p'(0) = f(h)$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akt9KA8_xehP"
      },
      "source": [
        "### 2.3. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajljvcjKxehP"
      },
      "source": [
        "### 2.3. Solution\n",
        "\n",
        "Let $f(x) = x^{T}Ax=\\:<x, Ax>$ \n",
        "\n",
        "then:\n",
        "\n",
        "$\\nabla f(x)[h]=\\nabla<x, Ax>[h] = \\: <h, Ax> + <x, Ah> \\: =\\\\\n",
        "<Ax+A^{T}x, h>$\n",
        "\n",
        "therfore: $ \\nabla f(x) = Ax+A^{T}x$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfUiDxHixehP"
      },
      "source": [
        "### 2.4. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{A} \\boldsymbol{X} \\right\\} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwF9K5brxehQ"
      },
      "source": [
        "### 2.4. Solution\n",
        "\n",
        "Let $f(X) = Tr\\{X^{T}AX\\} = \\: <X, AX>$ \n",
        "\n",
        "then:\n",
        "\n",
        "$\\nabla f(X)[H]=\\nabla<X, AX>[H] = \\: <H, AX> + <X, AH> \\: =\\\\\n",
        "<AX+A^{T}X, H>$\n",
        "\n",
        "therfore: $ \\nabla f(X) = AX+A^{T}X$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFoETvbqxehQ"
      },
      "source": [
        "### 2.5. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMYkV4nIxehQ"
      },
      "source": [
        "### 2.5. Solution\n",
        "\n",
        "Let $f(x) = ||y-Ax||^{2}_{2} = g_2(g_1(x))$ \n",
        "\n",
        "where:\n",
        "\n",
        "$g_1(x) = y-Ax$   \n",
        "\n",
        "$g_2(x) = ||x||^2_2$\n",
        "\n",
        "and:\n",
        "\n",
        "$\\nabla g_1(x)[h] = -Ah $   \n",
        "\n",
        "$\\nabla g_2(x) = 2x$\n",
        "\n",
        "then:\n",
        "\n",
        "$ \\nabla f(x)[h]= \\ <\\nabla g_2(g_1(x)), \\nabla(g_1(x)[h]> \\ =\\ <-2A^{T}(y-Ax), h>$\n",
        "\n",
        "therfore: $\\nabla f(x) = -2A^{T}(y-Ax)$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D86NHcFfxehQ"
      },
      "source": [
        "### 2.6. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X} \\right\\|}_{F}^{2} $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{D \\times d}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$.\n",
        " - ${\\left\\| \\cdot \\right\\|}_{F}^{2}$ is the squared [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm), that is, ${\\left\\| \\boldsymbol{X} \\right\\|}_{F}^{2} = \\left\\langle \\boldsymbol{X}, \\boldsymbol{X} \\right\\rangle = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{X} \\right\\}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCvJUtV6xehR"
      },
      "source": [
        "### 2.6. Solution\n",
        "\n",
        "Let $f(X) = ||Y-AX||^{2}_{F} = g_2(g_1(X))$ \n",
        "\n",
        "where:\n",
        "\n",
        "$g_1(X) = X-AX$   \n",
        "\n",
        "$g_2(X) = ||X||^2_F$\n",
        "\n",
        "and:\n",
        "\n",
        "$\\nabla g_1(X)[H] = -AH $   \n",
        "\n",
        "$\\nabla g_2(X)[H] =\\ <H, X> + <X, H>\\ =\\ <2X, H>$\n",
        "\n",
        "then:\n",
        "\n",
        "$ \\nabla f(X)[H]= \\ <\\nabla g_2(g_1(X)), \\nabla(g_1(X)[H]> \\ =\\ <-2A^{T}(Y-AX), H>$\n",
        "\n",
        "therfore: $\\nabla f(x) = -2A^{T}(Y-AX)$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKyKx4-mxehR"
      },
      "source": [
        "### 2.7. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{X}^{T} \\boldsymbol{A}, \\boldsymbol{Y}^{T} \\right\\rangle $$\n",
        "\n",
        "Where $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times D}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXrKkejrxehR"
      },
      "source": [
        "### 2.7. Solution\n",
        "\n",
        "$\\nabla f(X)[H] =\\ <H^{T}A, Y^{T}>\\ =\\ Tr(A^{T}HY^{T}) =\\ Tr(Y^{T}A^{T}H) = \\\\\n",
        "<(Y^{T}A^{T})^T, H>\\ =\\ <AY, H>$ \n",
        "\n",
        "therfore: $\\nabla f(X) = AY$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0d-HSPqxehR"
      },
      "source": [
        "### 2.8. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) $$\n",
        "\n",
        "Where $g \\left( \\cdot \\right)$ is an element wise function $g \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ie0ehuvFxehR"
      },
      "source": [
        "### 2.8. Solution\n",
        "\n",
        "$ \\nabla f({x})[h] = \\nabla \\left\\langle {a}^{T}, g({x}) \\right\\rangle [h] = 0 + \\left\\langle {a}^{T}, g'({x})[h] \\right\\rangle $ \n",
        "\n",
        "therefore:\n",
        "\n",
        "$ \\nabla f({x})[h] = \\left\\langle g'({x})^{T}{a}, h \\right\\rangle \\implies $ \n",
        "$ \\nabla f({x}) = g'({x})^{T}{a} $\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poIfkP34xehR"
      },
      "source": [
        "### 2.9. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{A}, \\log \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
        " - The function $\\log \\left( \\cdot \\right)$ is the element wise $\\log$ function: $\\boldsymbol{M} = \\log \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\log \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rcEU_TrjxehS"
      },
      "source": [
        "### 2.9. Solution\n",
        "\n",
        "$ Let: g({X}) = log({X}) \\implies g'({X}) = \n",
        "\\begin{bmatrix} \n",
        "{\\frac 1 {{x}_{11}}} & \\dots & {\\frac 1 {{x}_{1d}}} \\\\\n",
        "\\vdots & & \\vdots \\\\ \n",
        "{\\frac 1 {{x}_{d1}}} & \\dots & {\\frac 1 {{x}_{dd}}}\n",
        "\\end{bmatrix} $\n",
        "\n",
        "$ \\nabla f({x})[h] = \\nabla \\left\\langle {A}, log({X}) \\right\\rangle [h] = 0 + \\left\\langle {A}, g'({X})[h] \\right\\rangle $ \n",
        "\n",
        "therefore:\n",
        "\n",
        "$ \\nabla f({X})[h] = \\left\\langle g'({X})^{T}{A}, h \\right\\rangle \\implies $ \n",
        "$ \\nabla f({X}) = g'({X})^{T}{A} $\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgwZ6gxNxehS"
      },
      "source": [
        "### 2.10. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
        " - The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6S9aPhMYxehS"
      },
      "source": [
        "### 2.10. Solution\n",
        "\n",
        "$ Let: g({X}) = Diag({X}) \\implies g'({X}) = \n",
        "\\begin{bmatrix} \n",
        "\\frac {\\partial Diag(X)}{\\partial {x}_{11}} \\\\\n",
        "\\vdots \\\\ \n",
        "\\frac  {\\partial Diag(X)} {\\partial  {x}_{dd}}\n",
        "\\end{bmatrix} $\n",
        "\n",
        "$ \\nabla f({x})[h] = \\nabla \\left\\langle {a}, Diag({X}) \\right\\rangle [h] = 0 + \\left\\langle {a}, g'({X})[h] \\right\\rangle $ \n",
        "\n",
        "therefore:\n",
        "\n",
        "$ \\nabla f({X})[h] = \\left\\langle g'({X})^{T}{a}, h \\right\\rangle \\implies $ \n",
        "$ \\nabla f({X}) = g'({X})^{T}{a} $\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJmyVhYLxehS"
      },
      "source": [
        "## 3. Constraint Optimization\n",
        "\n",
        "**MinMax**  \n",
        "\n",
        "Let $G \\left( x, y \\right) = \\sin \\left( x + y \\right)$.\n",
        "\n",
        "### 3.1. Question\n",
        "\n",
        "Show that:\n",
        "\n",
        " - $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$.\n",
        " - $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgwxkRXUxehT"
      },
      "source": [
        "### 3.1. Solution\n",
        "\n",
        "\n",
        "\n",
        "*   for fixed x: $\\underset{y}{\\max} G \\left( x, y \\right) = 1$. \n",
        "\n",
        "therfore: $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = \\underset{x}{\\min}(1) = 1$.\n",
        "\n",
        "\n",
        "*   for fixed y: $\\underset{x}{\\min} G \\left( x, y \\right) = -1$. \n",
        "\n",
        "therfore: $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = \\underset{y}{\\max}(-1) = -1$.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jknza9xLxehT"
      },
      "source": [
        "**Rayleigh Quotient**  \n",
        "\n",
        "The _Rayleigh Quotient_ is defined by:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
        "\n",
        "For some symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$.\n",
        "\n",
        "### 3.2. Question\n",
        "\n",
        "Follow the given steps:\n",
        "\n",
        " - Show that $ {\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) = \\begin{cases} {\\min}_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases} $.\n",
        " - Write the Lagrangian of the constraint objective $\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right)$.\n",
        " - Show that ${\\nabla}_{\\boldsymbol{x}} \\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x}$.  \n",
        "   In other words, the stationary points $\\left( \\boldsymbol{x}, \\lambda \\right)$ are the eigenvectors and eigenvalues of $\\boldsymbol{A}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a7_ybsmRxehT"
      },
      "source": [
        "### 3.2. Solution\n",
        "\n",
        "\n",
        "Assuming: $ {\\min}_{x}f({x}) = {x}^{*} $\n",
        "\n",
        "\n",
        "Let's define: $ c \\in \\mathbb{R}$   , $ {\\left\\|c{x}^{*}\\right\\|}_{2}^{2} = 1 $\n",
        "\n",
        "Therfore:\n",
        "\n",
        "$ \\frac{{{x}^{*}}^{T}A{x}^{*}}{{{x}^{*}}^{T}{x}^{*}} = \\frac{{{x}^{*}}^{T}A{x}^{*}}{\\left\\|{x}^{*}\\right\\|_{2}^{2}} = \\frac{{c}^{2}\\left\\langle{x}^{*},{A{x}^{*}}\\right\\rangle}{{c}^{2}\\left\\langle{x}^{*},{x}^{*}\\right\\rangle} = \\frac{\\left\\langle{cx}^{*},{A{cx}^{*}}\\right\\rangle}{\\left\\langle{cx}^{*},{cx}^{*}\\right\\rangle} = \\frac{({c{x}^{*}})^{T}A({c{x}^{*}})}{({c{x}^{*}})^{T}({c{x}^{*}})} \\implies $ we can normzlize any min vector to 1\n",
        "\n",
        "\n",
        "\n",
        "Therefore $ {\\min}_{x}f({x}) = \\begin{cases} {\\min}_{x}f({x}) \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases} = \\begin{cases} {\\min}_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases}$\n",
        "\n",
        "\n",
        "$\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = {x}^{T}{A}{x} - {\\lambda}({{\\left\\|{x}\\right\\|}_{2}^{2} - 1}) $\n",
        "\n",
        "$ {\\nabla}_{x}{\\mathcal{L}}({x},{\\lambda}) = 2{A}{x} - 2{\\lambda}{x} $\n",
        "\n",
        "From here, it's easy to see ${\\nabla}_{\\boldsymbol{x}} \\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x}$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGJAWhJ9xehU"
      },
      "source": [
        "<img src=\"https://i.imgur.com/qIP5xPv.png\" height=\"700\">"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
